{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path = /root/src/aleister/log/logging.log\n"
     ]
    }
   ],
   "source": [
    "from master import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'test004'\n",
    "\n",
    "# args=['--symbol','BTC', '--execute_mode',\"prepro\",\n",
    "#       '--model_id',model_id,'--model_name','slstm',\n",
    "#       '--config_source','ini',\n",
    "#       '--user',\"Misaka00001\", '--source','ipynb',\n",
    "#       \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "#       '-train_sd','20210101','-train_ed','20210103',\n",
    "#       '-valid_sd','20210104','-valid_ed','20210105',\n",
    "#       '-test_sd','20210106','-test_ed','20210107']\n",
    "\n",
    "# args=['--symbol','BTC', '--execute_mode',\"train\",\n",
    "#       '--model_id',model_id,'--model_name','slstm',\n",
    "#       '--config_source','ini',\n",
    "#       '--user',\"Misaka00001\", '--source','ipynb',\n",
    "#       \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "#       '-train_sd','20210101','-train_ed','20210103',\n",
    "#       '-valid_sd','20210104','-valid_ed','20210105',\n",
    "#       '-test_sd','20210106','-test_ed','20210107']\n",
    "\n",
    "# args=['--symbol','BTC', '--execute_mode',\"gtrain\",\n",
    "#       '--model_id',model_id,'--model_name','slstm',\n",
    "#       '--config_source','ini',\n",
    "#       '--user',\"Misaka00001\", '--source','ipynb',\n",
    "#       \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "#       '-train_sd','20210101','-train_ed','20210103',\n",
    "#       '-valid_sd','20210104','-valid_ed','20210105',\n",
    "#       '-test_sd','20210106','-test_ed','20210107']\n",
    "\n",
    "# args=['--symbol','BTC', '--execute_mode',\"deploy_model\",\n",
    "#       '--model_id',model_id,'--model_name','slstm',\n",
    "#       '--config_source','ini',\n",
    "#       '--user',\"Misaka00001\", '--source','ipynb',\n",
    "#       \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "#       '-train_sd','20210101','-train_ed','20210103',\n",
    "#       '-valid_sd','20210104','-valid_ed','20210105',\n",
    "#       '-test_sd','20210106','-test_ed','20210107']\n",
    "\n",
    "\n",
    "args=['--symbol','BTC', '--execute_mode',\"rpredict\",\n",
    "      '--model_id',model_id,'--model_name','slstm',\n",
    "      '--config_source','ini',\n",
    "      '--user',\"Misaka00001\", '--source','ipynb',\n",
    "      \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "      '-train_sd','20210101','-train_ed','20210103',\n",
    "      '-valid_sd','20210104','-valid_ed','20210105',\n",
    "      '-test_sd','20210106','-test_ed','20210107']\n",
    "\n",
    "# args=['--symbol','BTC', '--execute_mode',\"train\",\n",
    "#       '--model_id','test002','--model_name','cgm',\n",
    "#       '--config_source','ini',\n",
    "#       '--user',\"Misaka00001\", '--source','ipynb',\n",
    "#       \"--general_config_mode\", \"DOCKER\",\"--private_api_mode\",\"DEFAULT\",\n",
    "#       '-train_sd','20210101','-train_ed','20210103',\n",
    "#       '-valid_sd','20210104','-valid_ed','20210105',\n",
    "#       '-test_sd','20210106','-test_ed','20210107']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path = /root/src/aleister/log/logging.log\n",
      "Log path = /root/src/aleister/log/logging.log\n"
     ]
    }
   ],
   "source": [
    "parser = make_parser()\n",
    "arg_dict = vars(parser.parse_args(args))\n",
    "\n",
    "# meta\n",
    "_id = arg_dict[\"model_id\"]\n",
    "general_config_mode = arg_dict[\"general_config_mode\"].upper()\n",
    "private_api_mode = arg_dict[\"private_api_mode\"].upper()\n",
    "sym = arg_dict[\"symbol\"]\n",
    "\n",
    "model_name = arg_dict[\"model_name\"].upper()\n",
    "\n",
    "om = OperateMaster()\n",
    "om.load_meta(_id, model_name, sym, general_config_mode, private_api_mode)\n",
    "om.init_prepro()\n",
    "\n",
    "# mlflow\n",
    "mlflow_tags = {\n",
    "    MLFLOW_USER: arg_dict[\"user\"],\n",
    "    MLFLOW_SOURCE_NAME: arg_dict[\"source\"],\n",
    "    MLFLOW_RUN_NAME: f\"TRAIN_{dt.now().strftime('%y%m%d%H%M%s')}\"\n",
    "}\n",
    "mlflow_client_kwargs = {\n",
    "    \"tracking_uri\": os.environ['MLFLOW_TRACKING_URI']\n",
    "}\n",
    "om.set_mlflow_settings(mlflow_client_kwargs, mlflow_tags)\n",
    "\n",
    "# load conofigs into each module\n",
    "om.init_learning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path = /root/src/aleister/log/logging.log\n",
      "Log path = /root/src/aleister/log/logging.log\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if arg_dict[\"execute_mode\"] == \"prepro\":\n",
    "    pass\n",
    "    om.init_dataGen()\n",
    "    # period\n",
    "    train_start = arg_dict[\"train_start_date\"] if \"train_start_date\" in arg_dict.keys() else None\n",
    "    train_end = arg_dict[\"train_end_date\"] if \"train_end_date\" in arg_dict.keys() else None\n",
    "    valid_start = arg_dict[\"valid_start_date\"] if \"valid_start_date\" in arg_dict.keys() else None\n",
    "    valid_end = arg_dict[\"valid_end_date\"] if \"valid_end_date\" in arg_dict.keys() else None\n",
    "    test_start = arg_dict[\"test_start_date\"] if \"test_start_date\" in arg_dict.keys() else None\n",
    "    test_end = arg_dict[\"test_end_date\"] if \"test_end_date\" in arg_dict.keys() else None\n",
    "    # om.preprocessing(sym, train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "else:\n",
    "    if arg_dict[\"execute_mode\"] == \"train\":\n",
    "        om.init_learning()\n",
    "        # om.train()\n",
    "    elif arg_dict[\"execute_mode\"] == \"gtrain\":\n",
    "        om.init_learning()\n",
    "        # om.gtrain()\n",
    "    elif arg_dict[\"execute_mode\"] == \"deploy_model\":\n",
    "        om.deploy_best_model()\n",
    "    elif arg_dict[\"execute_mode\"] == \"rpredict\":\n",
    "        om.init_dataGen()\n",
    "        # om.realtime_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = om.le\n",
    "fp = om.fp\n",
    "dg = om.dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om.preprocessing(sym, train_start, train_end, valid_start, valid_end, test_start, test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp._logger.info(f\"{train_start}, {train_end}, {valid_start}, {valid_end}, {test_start}, {test_end}\")\n",
    "fetch_start = min([_date for _date in [train_start, train_end, valid_start, valid_end, test_start, test_end] if\n",
    "                   _date is not None])\n",
    "fetch_end = max([_date for _date in [train_start, train_end, valid_start, valid_end, test_start, test_end] if\n",
    "                 _date is not None])\n",
    "# dg.get_hist_data(ch=\"trade\", sym=\"BTC\", sd=\"20220101\", ed=\"20220107\")\n",
    "\n",
    "#TODO: outside\n",
    "fetch_start=train_start\n",
    "fetch_end=test_end\n",
    "output_data_structure =fp.model_config.get(\"OUTPUT_DATA_STRUCTURE\")\n",
    "input_data_structure = fp.model_config.get(\"INPUT_DATA_STRUCTURE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flatten_v1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tree =dg.get_Xy(mode='train',input_data_structure=input_data_structure,output_data_structure=output_data_structure,sym=sym, sd=fetch_start, ed=fetch_end)\n",
    "fp._logger.info(\"[DONE] Get prepro raw datas. {0}~{1}\".format(fetch_start, fetch_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split train/test/val ineach leaf\n",
    "term_splited_data_tree = fp.split_child_by_period(data_tree,train_start, train_end, valid_start, valid_end, test_start, test_end)\n",
    "fp._logger.info(\"[DONE] Split by term.\")\n",
    "\n",
    "# Make each period tree\n",
    "train_datas = fp.select_child_by_period(term_splited_data_tree, 'train')\n",
    "test_datas = fp.select_child_by_period(term_splited_data_tree, 'test')\n",
    "val_datas = fp.select_child_by_period(term_splited_data_tree, 'valid')\n",
    "fp._logger.info(\"[DONE] picked by term.\")\n",
    "\n",
    "# to ndarray objects\n",
    "train_datas = fp.get_input_structures(input_data_structure)(train_datas)\n",
    "test_datas = fp.get_input_structures(input_data_structure)(test_datas, scalers=train_datas['scaler'])\n",
    "val_datas = fp.get_input_structures(input_data_structure)(val_datas, scalers=train_datas['scaler'])\n",
    "fp._logger.info(\"[DONE] To numpy and numpy \")\n",
    "\n",
    "#  Note: If other data except  for X is using. Fix X_train\": X_trains, \"X_val\":X_vals, \"X_test\": X_tests\n",
    "fp.save_numpy_datas(**{\n",
    "    \"X_train\": train_datas['data']['X'], \"X_val\": test_datas['data']['X'], \"X_test\": val_datas['data']['X'],\n",
    "    \"y_train\": train_datas['data']['y'], \"y_val\": test_datas['data']['y'], \"y_test\": val_datas['data']['y'],\n",
    "    \"X_scaler\": train_datas['scaler'],\"note\":{'input_dim':train_datas['input_dim']}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "obj_keys = [\"X_train\", \"X_val\", \"y_train\", \"y_val\", \"note\"]\n",
    "X_trains, X_vals, y_trains, y_vals, note = fp.load_numpy_datas(obj_keys)\n",
    "om.train_worker(X_trains, X_vals, y_trains, y_vals,  note )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99979973e-01, 5.17890703e-06, 1.49594198e-05],\n",
       "       [9.99979854e-01, 5.17760282e-06, 1.49675225e-05],\n",
       "       [9.99979854e-01, 5.16807131e-06, 1.50279675e-05],\n",
       "       ...,\n",
       "       [1.07656269e-05, 3.91482763e-06, 9.99985337e-01],\n",
       "       [1.08758677e-05, 3.94127801e-06, 9.99985218e-01],\n",
       "       [9.99778450e-01, 1.91520576e-05, 2.02424315e-04]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3420: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:188: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3420: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:188: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_228/2492342056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/src/master.py\u001b[0m in \u001b[0;36mgtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mlflow_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLFLOW_RUN_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{original_run_name}_{i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mlflow_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlflow_client_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlflow_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_out_of_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/master.py\u001b[0m in \u001b[0;36mtrain_worker\u001b[0;34m(self, X_trains, X_vals, y_train, y_val, note)\u001b[0m\n\u001b[1;32m    170\u001b[0m         self.le.train(train_loader, val_loader,\n\u001b[1;32m    171\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                       n_features=1)\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/learning_executor.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, batch_size, n_epochs, n_features)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m#  stacked numpy data  to nu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0mtruths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "om.gtrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train worker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_trains, X_vals, y_train, y_val, note = X_trains, X_vals, y_trains, y_vals, note\n",
    "# train set up\n",
    "model_params = {_k: le.hparams[_k] for _k in le.hparams[\"structure_params\"]}\n",
    "model_params['input_dim'] = note['input_dim']\n",
    "\n",
    "le.get_model_instance(le.model_name, model_params)\n",
    "\n",
    "fp.get_dataset_fn(le.hparams[\"dataset\"])\n",
    "dataset_params = {_k: le.hparams[_k] for _k in le.hparams[\"dataset_params\"]}\n",
    "train_loader, val_loader, _, _ = fp.get_dataloader(le.hparams[\"dataset\"], X_trains, y_train, X_vals,\n",
    "                                                        y_val, None, None, **dataset_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8c83641bbc8948f88acc2793f9e22e1a'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn_params = {}\n",
    "le.get_loss_fn(le.hparams[\"optimizer\"], {})\n",
    "\n",
    "optim_params = {\n",
    "    'params': le.model.parameters(),\n",
    "    'weight_decay': le.hparams[\"weight_decay\"],\n",
    "    'lr': le.hparams[\"lr\"]\n",
    "}\n",
    "\n",
    "le.get_optimizer(le.hparams[\"loss_fn\"], optim_params)\n",
    "\n",
    "# train\n",
    "le.train(train_loader, val_loader,\n",
    "              batch_size=le.hparams[\"batch_size\"], n_epochs=le.hparams[\"n_epoch\"],\n",
    "              n_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "n_epochs=10\n",
    "n_features=1\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'node_feature' and 'adj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125/3632725660.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batchs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batchs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batchs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/learning_executor.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, xs, ys)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'node_feature' and 'adj'"
     ]
    }
   ],
   "source": [
    "le._logger.info(\"[Start] Training. ID={0}\".format(le.id))\n",
    "# _ = le.get_model_save_path()  #todo : designate path\n",
    "\n",
    "# setup\n",
    "le.prediction = {}\n",
    "le.predictions_out = {}\n",
    "le.truths = {}\n",
    "le.truths_out = {}\n",
    "le.out_class = eval(le.model_config.get(\"OUT_CLASS\"))\n",
    "\n",
    "# mlflow\n",
    "dict_config = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"optimizer\": le.optimizer_name,\n",
    "    \"loss_fn\": le.loss_fn_name\n",
    "}\n",
    "\n",
    "le.mlwriter.create_experiment(le.id, le.mlflow_tags)\n",
    "le.mlwriter.log_params_from_omegaconf_dict(dict_config)\n",
    "\n",
    "# start\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    batch_losses = []\n",
    "    le.train_loader = train_loader\n",
    "    for x_batchs, y_batchs in train_loader:\n",
    "        le.optimizer.zero_grad()\n",
    "        loss = le.train_step(x_batchs, y_batchs)\n",
    "        batch_losses.append(loss)\n",
    "    training_loss = np.mean(batch_losses)\n",
    "    le.train_losses.append(training_loss)\n",
    "    le.mlwriter.log_metric('train_loss', training_loss, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_val_losses = []\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        for x_vals, y_val in val_loader:\n",
    "            prediction, truth, val_loss = le.eval_step(x_vals, y_val)\n",
    "            predictions.append(prediction)\n",
    "            truths.append(truth)\n",
    "            batch_val_losses.append(val_loss)\n",
    "\n",
    "        validation_loss = np.mean(batch_val_losses)\n",
    "        le.val_losses.append(validation_loss)\n",
    "        \n",
    "        predictions = np.concatenate(predictions)\n",
    "        truths = np.concatenate(truths)\n",
    "\n",
    "        # record\n",
    "        le.prediction[\"val\"], le.truths[\"val\"] = predictions, truths\n",
    "        predictions_out, truths_out = le.convert_model_value(predictions), le.convert_model_value(truths)\n",
    "        le.predictions_out[\"val\"], le.truths_out[\"val\"] = predictions_out, truths_out\n",
    "        acc = accuracy_score(truths_out, predictions_out)\n",
    "\n",
    "        le.mlwriter.log_metric('valid_acc', acc, epoch)\n",
    "        le.mlwriter.log_metric('valid_loss', validation_loss, epoch)\n",
    "        # print(le.prediction[\"val\"])\n",
    "        # print(le.predictions_out[\"val\"])\n",
    "\n",
    "    if (epoch <= 10) | (epoch % 50 == 0):\n",
    "        le._logger.info(\n",
    "            f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "# record last \n",
    "le.save_numpy_obj(predictions, \"val_preds.csv\")\n",
    "le.save_numpy_obj(truths, \"val_truth.csv\")\n",
    "le.save_numpy_obj(predictions_out, \"val_preds_out.csv\", type='int')\n",
    "le.save_numpy_obj(truths_out, \"val_truth_out.csv\", type='int')\n",
    "\n",
    "# le.save_model()\n",
    "le.save_mlflow_model()\n",
    "le._logger.info(\"[DONE] Training. ID={0}\".format(le.id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path = /root/src/aleister/log/logging.log\n"
     ]
    }
   ],
   "source": [
    "om.deploy_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b45f9670391769e061d21fde4a80e23c28183b0e668940abb0d6257953f3010d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
